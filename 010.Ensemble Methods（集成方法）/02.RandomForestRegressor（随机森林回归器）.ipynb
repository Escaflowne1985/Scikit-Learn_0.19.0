{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestRegressor\n",
    "(n_estimators='warn', criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=red>n_estimators</font>: integer, optional (default=10)  \n",
    "The number of trees in the forest.\n",
    "* <font color=red>criterion</font>: string, optional (default=”mse”)   \n",
    "用来测量分割质量的函数。支持的标准有均值平方误差的“mse”，等于方差约简作为特征选择标准;均值绝对误差的“mae”。\n",
    "* <font color=red>max_depth</font>: integer or None, optional (default=None)  \n",
    "树的最大深度。如果没有节点，则展开节点，直到所有叶子都是纯的，或者直到所有叶子包含的样本都小于min_samples_split。  \n",
    "* <font color=red>min_samples_split</font>: int, float, optional (default=2)  \n",
    "  - 分割内部节点所需的最小样本数量:  \n",
    "  int:将min_samples_split作为最小值。  \n",
    "  float:min_samples_split是一个分数，ceil(min_samples_split * n_samples)是每个分割的最小样本数。\n",
    "* <font color=red>min_samples_leaf</font>: int, float, optional (default=1)  \n",
    "叶节点上所需最小样本数。在左分支和右分支中至少留下min_samples_leaf训练样本时，会考虑任何深度的分割点。这可能有平滑模型的效果，特别是在回归中。 \n",
    "int:将min_samples_leaf作为最小值。  \n",
    "float:min_samples_leaf是一个分数，ceil(min_samples_leaf * n_samples)是每个节点的最小样本数。  \n",
    "* <font color=red>min_weight_fraction_leaf</font>: float, optional (default=0.)  \n",
    "叶节点上(所有输入样本的)权值之和的最小加权分数。当不提供sample_weight时，示例的权值相等。 \n",
    "* <font color=red>max_features</font>: int, float, string or None, optional (default=”auto”)\n",
    "  - 在寻找最佳分割时需要考虑的特性数量:  \n",
    "  int:考虑每个分割处的max_features特性。\n",
    "  float:然后max_features是一个分数，在每个分割中都考虑int(max_features * n_features)特性。 \n",
    "  “auto”:max_features=sqrt(n_features).\n",
    "  “sqrt”:max_features=sqrt(n_features) \n",
    "  “log2”:then max_features=log2(n_features).\n",
    "   None:max_features=n_features.  \n",
    "   注意:在找到节点样本的至少一个有效分区之前，即使需要有效地找到分区，也不会停止对分割的搜索检查最多的特征特征。\n",
    "* <font color=red>max_leaf_nodes</font>: int or None, optional (default=None)  \n",
    "以最佳优先方式种植带有max_leaf_node的树。最佳节点定义为杂质的相对还原。如果没有，那么叶子节点的数量是无限的。\n",
    "* <font color=red>min_impurity_decrease</font>: float, optional (default=0.)  \n",
    "  - 如果这种分裂导致杂质的减少大于或等于这个值，节点将被分裂。加权杂质降低方程为:  \n",
    "  N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)    \n",
    "  其中N为样本总数，N_t是当前节点上的样本数量，N_t_L是左子节点中的样本数量，N_t_R右子节点中的样本数量   \n",
    "  N, N_t, N_t_R and N_t_L 如果传递sample_weight，则所有都引用加权和。\n",
    "* <font color=red>min_impurity_split</font>: float, (default=1e-7)  \n",
    "树木生长早期停止的阈值。如果一个节点的杂质超过阈值，它就会分裂，否则它就是一个叶子。\n",
    "* <font color=red>bootstrap</font>: boolean, optional (default=True)  \n",
    "构建树时是否使用引导样本。\n",
    "* <font color=red>oob_score</font>: bool, optional (default=False)  \n",
    "是否使用out-of-bag样本对看不见的数据来估计R ^ 2。\n",
    "* <font color=red>n_jobs</font>: int or None, optional (default=None)  \n",
    "对于拟合和预测，并行运行的作业数量。没有一个意思是1，除非是在工作中。parallel_backend上下文。-1表示使用所有处理器。有关详细信息，请参见Glossary。\n",
    "* <font color=red>random_state</font>: int, RandomState instance or None, optional (default=None)  \n",
    "如果是int, random_state是随机数生成器使用的种子;如果是RandomState实例，random_state是随机数生成器;如果没有，随机数生成器是np.random使用的RandomState实例。\n",
    "* <font color=red>verbose</font>: int, optional (default=0)  \n",
    "在拟合和预测时控制详细程度。\n",
    "* <font color=red>warm_start</font>: bool, optional (default=False)  \n",
    "当设置为True时，重用上一个调用的解决方案进行匹配，并向集成中添加更多的估计器，否则，只需要匹配一个全新的forest。查看术语表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18146984 0.81473937 0.00145312 0.00233767]\n",
      "[-8.32987858]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_features=4, n_informative=2,\n",
    "                       random_state=0, shuffle=False)\n",
    "regr = RandomForestRegressor(max_depth=2, random_state=0,\n",
    "                             n_estimators=100)\n",
    "regr.fit(X, y)\n",
    "\n",
    "print(regr.feature_importances_)\n",
    "\n",
    "print(regr.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
