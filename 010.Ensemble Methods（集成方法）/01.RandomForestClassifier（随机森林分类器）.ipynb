{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier\n",
    "(n_estimators='warn', criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=red>n_estimators</font>: integer, optional (default=10)  \n",
    "The number of trees in the forest.\n",
    "* <font color=red>criterion</font>: string, optional (default=”gini”)  \n",
    "用来测量分割质量的函数。支持的标准是“基尼系数”表示基尼，“熵”表示信息增益。\n",
    "* <font color=red>max_depth</font>: integer or None, optional (default=None) \n",
    "树的最大深度。如果没有节点，则展开节点，直到所有叶子都是纯的，或者直到所有叶子包含的样本都小于min_samples_split。\n",
    "* <font color=red>min_samples_split</font>: int, float, optional (default=2)  \n",
    "  - 分割内部节点所需的最小样本数量:  \n",
    "  int:将min_samples_split作为最小值。  \n",
    "  float:min_samples_split是一个分数，ceil(min_samples_split * n_samples)是每个分割的最小样本数。\n",
    "* <font color=red>min_samples_leaf</font>: int, float, optional (default=1)  \n",
    "叶节点上所需最小样本数。在左分支和右分支中至少留下min_samples_leaf训练样本时，会考虑任何深度的分割点。这可能有平滑模型的效果，特别是在回归中。\n",
    "int:将min_samples_leaf作为最小值。  \n",
    "float:min_samples_leaf是一个分数，ceil(min_samples_leaf * n_samples)是每个节点的最小样本数。  \n",
    "* <font color=red>min_weight_fraction_leaf</font>: float, optional (default=0.)  \n",
    "叶节点上(所有输入样本的)权值之和的最小加权分数。当不提供sample_weight时，示例的权值相等。\n",
    "* <font color=red>max_features</font>: int, float, string or None, optional (default=”auto”)  \n",
    "  - 在寻找最佳分割时需要考虑的特性数量:  \n",
    "  int:考虑每个分割处的max_features特性。\n",
    "  float:然后max_features是一个分数，在每个分割中都考虑int(max_features * n_features)特性。 \n",
    "  “auto”:max_features=sqrt(n_features).\n",
    "  “sqrt”:max_features=sqrt(n_features) \n",
    "  “log2”:then max_features=log2(n_features).\n",
    "   None:max_features=n_features.  \n",
    "   注意:在找到节点示例的至少一个有效分区之前，对分割的搜索不会停止，即使它需要有效地检查max_features以外的特性。\n",
    "* <font color=red>max_leaf_nodes</font>: int or None, optional (default=None)  \n",
    "以最佳优先方式种植带有max_leaf_node的树。最佳节点定义为杂质的相对还原。如果没有，那么叶子节点的数量是无限的。\n",
    "* <font color=red>min_impurity_decrease</font>: float, optional (default=0.)    \n",
    "  - 如果这种分裂导致杂质的减少大于或等于这个值，节点将被分裂。加权杂质降低方程为:  \n",
    "  N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)  \n",
    "  其中N为样本总数，N_t是当前节点上的样本数量，N_t_L是左子节点中的样本数量，N_t_R右子节点中的样本数量  \n",
    "  N, N_t, N_t_R and N_t_L 如果传递sample_weight，则所有都引用加权和。\n",
    "* <font color=red>min_impurity_split</font>: float, (default=1e-7)  \n",
    "树木生长早期停止的阈值。如果一个节点的杂质超过阈值，它就会分裂，否则它就是一个叶子。\n",
    "* <font color=red>bootstrap</font>: boolean, optional (default=True)  \n",
    "构建树时是否使用引导样本。\n",
    "* oob_score : bool (default=False)  \n",
    "是否使用out-of-bag样本估计泛化精度。\n",
    "* <font color=red>n_jobs</font>: int or None, optional (default=None)\n",
    "对于拟合和预测，并行运行的作业数量。没有一个意思是1，除非是在工作中。parallel_backend上下文。-1表示使用所有处理器。有关详细信息，请参见Glossary。  \n",
    "* <font color=red>random_state</font>: int, RandomState instance or None, optional (default=None)\n",
    "如果是int, random_state是随机数生成器使用的种子;如果是RandomState实例，random_state是随机数生成器;如果没有，随机数生成器是np.random使用的RandomState实例。\n",
    "* <font color=red>verbose</font>: int, optional (default=0)\n",
    "在拟合和预测时控制详细程度。\n",
    "* <font color=red>warm_start</font>: bool, optional (default=False)\n",
    "当设置为True时，重用上一个调用的解决方案进行匹配，并向集成中添加更多的估计器，否则，只需要匹配一个全新的forest。查看术语表。\n",
    "* <font color=red>class_weight</font>: dict, list of dicts, “balanced”, “balanced_subsample” or None, optional (default=None)  \n",
    "与表单{class_label: weight}中的类关联的权重。如果没有给出，所有类的权值都应该是1。对于多输出问题，可以按照与y列相同的顺序提供一个dicts列表。  \n",
    "Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}].  \n",
    "“balanced”模式使用y的值作为n_samples / (n_classes * np.bincount(y))自动调整权重，权重与输入数据中的类频率成反比。  \n",
    "“balanced_subsample”模式与“balanced”模式相同，不同之处是权重是根据每棵树的引导样本计算的。  \n",
    "对于多输出，y的每一列的权值将相乘。  \n",
    "注意，如果指定sample_weight，这些权重将与sample_weight相乘(通过fit方法传递)。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14205973 0.76664038 0.0282433  0.06305659]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                             random_state=0)\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.feature_importances_)\n",
    "\n",
    "print(clf.predict([[0, 0, 0, 0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
